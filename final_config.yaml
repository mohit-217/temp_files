base_model: Qwen/Qwen2.5-3B
model_type: Qwen2ForCausalLM
tokenizer_type: Qwen2Tokenizer
is_llama_derived_model: false

# Special tokens configuration (as provided)
special_tokens:
  additional_special_tokens:
    - "<functions>"
    - "</functions>"
    - "<function_calls>"
    - "</function_calls>"
    - "<function>"
    - "</function>"
    - "<think>"
    - "</think>"
    - "<function name="

# Core hyperparameters tuned for best accuracy on complex tasks
# (Empirical evidence from LRBench++ and recent LLM fine-tuning studies suggest that a very low LR is key)
learning_rate: 8e-6
micro_batch_size: 1
gradient_accumulation_steps: 8
num_epochs: 1
weight_decay: 0.04
dropout: 0.03
warmup_ratio: 0.10
lr_scheduler: "cosine_with_restarts"
max_grad_norm: 1.0
chat_template: chatml
# Dataset configuration
datasets:
- path: json
  ds_type: json
  split: train
  type: chat_template
  data_files:
    - sampled_1000.jsonl
  trust_remote_code: True

# Data processing settings
sequence_len: 8192
pad_to_sequence_len: true
sample_packing: false

# Precision settings
bf16: true
tf32: true

# Optimization with Liger kernels for improved throughput and efficiency
flash_attention: true
optimization:
  use_liger: true
  flash_attention_implementation: "variant-3-liger"

# Special token handling during initialization
model_init_kwargs:
  init_device: "meta"
  resize_token_embeddings_strategy: "mean"

# DeepSpeed config
deepspeed: deepspeed_configs/zero1.json
  # zero_optimization:
  #   stage: 1
  #   overlap_comm: true
  #   contiguous_gradients: true
  # gradient_clipping: 1.0
  # bf16:
  #   enabled: true
  # steps_per_print: 100

# Evaluation and checkpointing
save_strategy: "steps"
save_steps: 500
eval_strategy: "steps"
eval_steps: 500
save_total_limit: 100
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false
early_stopping_patience: 5

# Validation settings
val_set_size: 0.05
do_eval: true

# WandB integration
wandb:
  enabled: true
dataset_prepared_path: /shared/last_run_prepared
output_dir: /shared/outputs/first_sample_training